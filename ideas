Pre processing.

to build the annotations on Kanji positions, create code that reads the manual Kanji masks and classifies the Kanji, then writes everything to a text file.




for postprocessing, 


- create code to combine masks (heatmap)

- make postprocess or evaluate file
	- receive the route of the results (txt boxes, binary masks) and of the original test set  
	- for the binary mask, use tooya's code to say how many where found "missed"
	- look for yolo code for map50... and do that with the boxes or masks

- test with images without noise reduction too
	
- implement leave one out 10 fold cross validation scheme

- How do we create a database for kanji classification ?
	- Ideally, use the same characters. Can we define what those characters are?
	- 
	
	
1) find Waki's scanned data DONE
1.5) find sanganus data
2) bash oneliner to turn folders into zips
3) adapt fixannotation to downlowad and read these zip files and create open document spreadsheets for each, email antonia
4) noise processing
4.1) find code to filter line regions
4.2) add to sauvola code
4.3) make noise reduction protocol
4.4) run with the whole dataset



Mail Antonia



https://colab.research.google.com/drive/11Z7CC-b2ZBJLub-RJqW5jg_IEFoL627x?usp=sharing



